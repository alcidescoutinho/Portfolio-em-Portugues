<h1>Web Scraping: Um futuro cientista de dados</h1>
<h3>Problematica</h3>
Nesse trabalho de análise de dados foi simulado que um jovem cientista de dados está em busca do seu primeiro emprego na área, mas ele encontra certas limitações na sua 
busca, já que procura uma um trabalho remoto, então ele decide fazer um algoritmo que tenha as seguintes funcionalidades, como:

- Coletar os dados diretamente no site, sendo esses dados:
  - Local de Trabalho
  - Nome da empresa
  - Descrição do trabalho (com informações do que é exigido)
  - Como a vaga está sendo ofertada
  - Informações da empresa
  - link para se cadastrar
- Trabalho no tratamento de dados
- Fazer um levantamento visual para facilitar a sua busca por essa vaga.

<h3>Objetivos</h3>
Com os dados em mãos o jovem cientista de dados busca responder algumas indagações, sendo-as:

- Visão geral dos dados 
  - Quantas vagas foram ofertadas nas duas ultimas semanas?
  - Visualização geral dessas vagas (Local de trabalho, Nivel de experiência da vaga, avaliações das empresas)
  - Média das notas dessas avaliações
  - Se só há ofertas para cientista de dados
 
 
 - Visão específica dos dados
   - Estudo mais preciso sobre as vagas para trabalho remoto.
   - Montar um mapa de calor das ofertas de vagas para trabalho remoto.
    - Selecionar as 3 melhores vagas com alguns critérios (Sendo um dos principais o crescimento dentro da empresa)


<h3>Informações Técnicas</h3>
Para facilitar a vida do leitor, aqui estão os temas e bibliotecas abordados na elaboração desse trabalho.

- Plataforma: Google Colaboratory.
- Web Scraping: Utilizado a biblioteca **Selenium** para a coleta dos dados e a **BeautifulSoup** para a localização de algumas informações.
- Texto: **ReGex** 
- Padrões: **Pandas, Numpy,  Seaborn, Matplotlibe e time.** 

